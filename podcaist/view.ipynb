{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a “thinking-aloud” style walk-through of how I examined the experimental section of “Learning Physically Simulated Tennis Skills from Broadcast Videos” (TOG ’23) and judged whether the empirical evidence really supports each of the authors’ high-level claims.\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "1. What do the authors actually claim?\n",
      "--------------------------------------------------------------------\n",
      "Scanning the abstract, the teaser (Fig. 1) and the bullet list in Sec. 1, the paper makes four concrete promises:\n",
      "\n",
      "C1  Diverse, physics-plausible tennis skills (serve, 1-/2-handed BH, FH, topspin, slice, left/right handedness) learned from broadcast video with *no* stroke annotations.\n",
      "\n",
      "C2  The learned player can place the ball accurately—i.e. “hit the incoming ball to target positions with high success”.\n",
      "\n",
      "C3  Two such players can rally for long stretches.\n",
      "\n",
      "C4  All of the above is obtained through a simple hierarchical RL pipeline in which (i) noisy video poses are “fixed” by a low-level physics tracker, and (ii) a high-level controller plans in the learned motion-VAE latent space.  \n",
      "     → therefore (C4) can be split into two sub-claims  \n",
      "       •  the physics-based correction (LL policy) is *necessary*  \n",
      "       •  the hybrid wrist override is *necessary*.\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "2. What quantitative evidence is provided?\n",
      "--------------------------------------------------------------------\n",
      "The entire quantitative section (Sec. 9) boils down to five tables and one million-shot analysis plot.\n",
      "\n",
      "A) “Task performance” metrics, reported on 10 000 evaluation sessions (each session = 15 balls = 150 000 balls per model).\n",
      "   • Hit rate – fraction of balls the racket touches.  \n",
      "   • Bounce-in rate – fraction of those hits that land inside opponent’s court.  \n",
      "   • Bounce-position err – Euclidean distance (m) between desired and actual 1st bounce.\n",
      "\n",
      "   → Reported as 25/50/75 % quantiles in Table 1.\n",
      "\n",
      "B) “Motion quality” metrics.\n",
      "   • Jitter – mean third derivative of all joint positions (10-3 m/s³).  \n",
      "   • Foot-sliding – mean per-frame displacement of grounded vertices (cm).\n",
      "\n",
      "   → Table 3 (raw vs physics-corrected vs after MVAE vs final) and Table 5 (effect of residual forces).\n",
      "\n",
      "C) Ablations (Table 2, Table 4) remove PhysicsCorr, HybridCtr, phase label, future ball observation, curriculum, etc.\n",
      "\n",
      "D) A one-million-shot analysis (Fig. 5) visualises how hit-rate / bounce-err vary with incoming speed, spin, bounce-position, reaction distance.\n",
      "\n",
      "There is *no* direct metric for “diversity of strokes”, “realism of joint torques”, “style similarity to Federer”, or “rally length distribution”.\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "3. Do the numbers substantiate each claim?\n",
      "--------------------------------------------------------------------\n",
      "C1 – “Diverse, realistic skills from video, no annotations.”\n",
      "\n",
      "• Diversity.  The paper supplies only *qualitative* evidence (Fig. 4 and supplementary video).  No classifier is run to show that all six stroke categories occur with reasonable frequency, nor a latent traversal study.  \n",
      "  ⇒ The claim is plausible but *not quantitatively demonstrated*.\n",
      "\n",
      "• Physical realism.  Jitter ↓ from 6.08 → 0.51, foot slide ↓ from 7.4 cm → 1.46 cm (Table 3) – that is a large improvement and the absolute numbers ( < 2 cm sliding) are on par with mocap-trained locomotion papers.  So “plausible” is believable.\n",
      "\n",
      "• Lack of annotations.  True by construction; however, the high-level reward *does* encode contact phase (π) and spin direction, each of which is manual signal.  Not a deal-breaker, but the paper understates the degree of supervision.\n",
      "\n",
      "C2 – “Accurate shot placement.”\n",
      "\n",
      "• Median Hit rate ≈ 0.92 and Bounce-in rate ≈ 0.81 (Table 1).  For tennis these are high: a pro rally hit-rate is ~0.98 but bounce-in after an aggressive target is lower, so 0.8 is respectable.  \n",
      "• Median bounce-err ≈ 1.4–1.7 m.  For a singles court (8.23 m half-width, 11.89 m depth) that is ~15–20 % of court length – not pinpoint, but solid.  \n",
      "• Fig. 5 shows error inflates near sidelines/deep baseline, mirroring human difficulty.\n",
      "\n",
      "⇒ Claim basically supported, though “accurate” should be read as “within ~2 m”.\n",
      "\n",
      "C3 – “Extended rallies.”\n",
      "\n",
      "• Only anecdotal evidence: a 38-shot rally (∼41 s) between Fed-full & Djo-full. No histograms of rally length, success rate, or comparison to self-play baselines.  \n",
      "⇒ Claim is *weakly* backed; a single cherry-picked rally does not prove robustness.\n",
      "\n",
      "C4 – “Necessity of physics correction + hybrid wrist.”\n",
      "\n",
      "• Table 2 ablation:  \n",
      "  w/o PhysicsCorr → bounce-err +39 %, slide +93 % → clearly valuable.  \n",
      "  w/o HybridCtr   → bounce-in halved (0.81→0.46) → wrist correction definitely critical.  \n",
      "\n",
      "• Table 3 shows progressive jitter/sliding reductions from M_kin → M_corr → M_VAE → Fed-full.\n",
      "\n",
      "⇒ Evidence strongly supports the *importance* of the two components.\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "4. Are the chosen metrics sufficient or lacking?\n",
      "--------------------------------------------------------------------\n",
      "Pros\n",
      "• Hit/Bounce-in/Bounce-err are intuitive, task-centric, easy to verify visually.  \n",
      "• Jitter & foot-sliding are standard for physics-based character work.  \n",
      "• Large-scale eval (10 k × 15 = 150 k balls) and 1 M-shot stress-test give statistical confidence.\n",
      "\n",
      "Cons / things left to be desired\n",
      "1. **Stroke-type quantification.**  No metric to show that the agent *indeed* performs slices, topspin, 1-hand BH, 2-hand BH etc. The spin reward merely enforces topspin vs backspin, not the kinematics of a slice.\n",
      "\n",
      "2. **Opponent-aware strategy.**  Bounce-in rate measures legality, not quality of placement (e.g. depth, angle relative to opponent).  Rally success is not rigorously evaluated.\n",
      "\n",
      "3. **Style similarity.**  Claims of “left-handed Nadal, one-handed Federer” are not quantified (no cosine similarity in latent, no classifier).  Given some visible mismatches (short follow-through, absent non-dominant-hand placement) a metric such as FID-pose or MT-VAE log-likelihood would have strengthened the claim.\n",
      "\n",
      "4. **Physical plausibility beyond kinematics.**  They do not report joint torque magnitudes, energy expenditure, or compliance with joint-limit forces.  Residual force ablation shows a 40 % slide reduction, but the absolute magnitude of residuals vs typical human muscle limits is not given.\n",
      "\n",
      "5. **Baseline comparison.**  The paper does not compare to prior “RL from mocap” or “Video imitation” baselines on the *same* tennis task.  Hence it is hard to know if a 1.7 m error is a breakthrough or merely adequate.\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "5. Bottom-line evaluation\n",
      "--------------------------------------------------------------------\n",
      "• For *control-oriented* claims (C2, C4) the metrics are sensible and the numbers convincing.  \n",
      "• For *diversity/stylistic* claims (C1) and *rally robustness* (C3) the evidence is largely qualitative; additional quantitative metrics would strengthen confidence.\n",
      "\n",
      "If I were reviewing, I would say:  \n",
      "“Good task-level metrics, but please add a per-stroke confusion matrix, rally-length distribution, and maybe a learned style-similarity score to fully close the loop on the ‘diverse human-like play’ promise.”\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = \"/Users/derek/Desktop/podcaist/podcaist/saved_outputs/results_o3-2025-04-16.json\"\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens=11 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "\n",
    "client = genai.Client(api_key=\"AIzaSyDkXnh2V9omq9yGu52ifeaE-116Mlqz2ik\")\n",
    "\n",
    "model = \"gemini-2.5-pro\n",
    "\n",
    "response = client.models.count_tokens(\n",
    "    model=model,\n",
    "    contents=[(\"text\", \"Hey how many tokens are in this prompt?\")],\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
